{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"What are some ways to extract information from a website that doesn't have an export option?\"\n",
    "\n",
    "The reason behind this question was my need to keep a watchful eye on product prices listed on Flipkart and Amazon, and be notified when the prices hit their lowest point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that others might also require to monitor product prices from websites, I decided to share a Python program that emulates web browsing behavior, enables easy data storage and retrieval, and can even send alerts via email when prices reach a minimum from first day of monitoring."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, let's import several Python modules and packages that will be used in the script:\n",
    "\n",
    "1. bs4 (BeautifulSoup): a Python library used for web scraping to parse HTML and XML documents.\n",
    "2. requests: a Python library used to send HTTP requests to websites and retrieve the response data.\n",
    "3. smtplib: a Python library used for sending email messages using Simple Mail Transfer Protocol (SMTP).\n",
    "4. csv: a Python library used for reading and writing CSV (Comma Separated Values) files.\n",
    "5. datetime: a Python library used to work with dates and times.\n",
    "6. os.path: a Python library used for working with file paths.\n",
    "7. MIMEText: a Python library used to create email messages in MIME (Multipurpose Internet Mail Extensions) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import smtplib\n",
    "import csv \n",
    "import datetime\n",
    "import os.path\n",
    "from email.mime.text import MIMEText"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the URLs for the product pages on Flipkart and Amazon in variables for easy reference in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flipkart_URL = 'https://www.flipkart.com/logitech-gamepad-f310/p/itmdyfyfrzch29cf?pid=ACCDYFXZ6QEUZEFZ&lid=LSTACCDYFXZ6QEUZEFZIISO8W&marketplace=FLIPKART&q=controller&store=4rr%2Fkm5%2Fr39%2Fa7g%2Fy7a&spotlightTagId=TrendingId_4rr%2Fkm5%2Fr39%2Fa7g%2Fy7a&srno=s_1_5&otracker=AS_QueryStore_OrganicAutoSuggest_1_9_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_9_na_na_na&fm=search-autosuggest&iid=01b3d6f3-06c0-449b-a2af-9d3fd8e91cbb.ACCDYFXZ6QEUZEFZ.SEARCH&ppt=sp&ppn=sp&ssid=mjbnhasl2o0000001678195519116&qH=594c103f2c6e04c3'\n",
    "Amazon_URL = 'https://www.amazon.in/Logitech-G-940-000112-F310-Gamepad/dp/B0757QFBRL/ref=sr_1_4?crid=YSVEB0DD6CAB&keywords=logitech+controller+for+pc&qid=1678195661&s=computers&sprefix=logitech+controller+for+pc%2Ccomputers%2C182&sr=1-4'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this program, I'm using the 'Requests: HTTP for Humans' library.\n",
    "As GET request to the URL will send headers.\n",
    "and since we are using python 'User Agent' header will be something like 'python-requests/2.19.1'.\n",
    "Basically this means someone is scraping the website using python.\n",
    "As some websites block such requests we will define headers to behave as humanly as possible.\n",
    "In order to look what headers are sent by browser please go to 'https://httpbin.org/get'.\n",
    "From above mentioned URL copy \"User-Agent\" tag and substitute below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about HTTP and Headers here 'https://www.seobility.net/en/wiki/HTTP_headers'.\n",
    "Read more about requests library 'https://requests.readthedocs.io/en/latest/'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the content of the website, we'll use the get() method from the requests library, which will allow us to establish a connection to the website and capture its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flipkart_page = requests.get(Flipkart_URL, headers = headers)\n",
    "Amazon_page = requests.get(Amazon_URL, headers = headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything contained in that webpage is now in the page object.\n",
    "In order to parse this (HTML)pages we will use Beautiful Soup (BS4) library. \n",
    "BS4 is a Python library for parsing HTML and XML documents. \n",
    "It provides simple methods to navigate, search, and modify parse trees."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page objects now contains all the content from the webpage we retrieved. To parse the HTML code contained within the webpage, we will use the Beautiful Soup (BS4) library. BS4 is a Python library specifically designed for parsing HTML and XML documents. It provides easy-to-use methods for navigating, searching, and modifying the parse tree of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flipkart_soup = BeautifulSoup(Flipkart_page.content, \"html.parser\")\n",
    "Amazon_soup = BeautifulSoup(Amazon_page.content, \"html.parser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BS4 provides a set of useful functions that simplify the process of traversing HTML tags. You can find a detailed documentation of all functions on the BS4 website at https://www.crummy.com/software/BeautifulSoup/bs4/doc/. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functions like find(), find_all(), and get_text() we will grab product title, availability and prices from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing product title\n",
    "\n",
    "Flipkart_Title = Flipkart_soup.find_all('span',attrs={\"class\":\"B_NuCI\"})[0].get_text()\n",
    "Flipkart_Title = Flipkart_Title.strip()\n",
    "\n",
    "Amazon_Title = Amazon_soup.find(id='productTitle').get_text()\n",
    "Amazon_Title = Amazon_Title.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, for the flipkart website, the code first searches for all 'span' elements that have a 'class' attribute of 'B_NuCI' using find_all() function.\n",
    "\n",
    "Similarly, for amazon website, the code locates an element with the 'id' attribute of 'availability' using find() function.\n",
    "\n",
    "The get_text() function is then used to extract the text content of this element.\n",
    "\n",
    "And .strip() function is used to remove any extra whitespace characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling Availability of the Product from website\n",
    "\n",
    "Flipkart_Availability = Flipkart_soup.find_all('div',attrs={\"class\":\"_16FRp0\"})\n",
    "\n",
    "if (len(Flipkart_Availability)==0):\n",
    "    Flipkart_Availability = 'In stock'\n",
    "else:\n",
    "    Flipkart_Availability = Flipkart_Availability[0].get_text()\n",
    "Flipkart_Availability = Flipkart_Availability.strip()\n",
    "\n",
    "Amazon_Availability = Amazon_soup.find(id='availability').get_text()\n",
    "Amazon_Availability = Amazon_Availability.strip()\n",
    "Amazon_Availability_Split = Amazon_Availability.lower().split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, for the Flipkart website, the code first searches for all 'div' elements that have a 'class' attribute of '_16FRp0' using the find_all() function. \n",
    "\n",
    "If the length of this list of elements is zero i.e. this 'div' has no text content, it means that the product is in stock, so the 'Flipkart_Availability' variable is assigned the string 'In stock'. \n",
    "\n",
    "If the length is not zero, the code retrieves the text from the first element in the list and assigns it to 'Flipkart_Availability' variable.\n",
    "\n",
    "The get_text() function is then used to extract the text content of this element.\n",
    "\n",
    "The .strip() function is used to remove any extra whitespace characters from the text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly,for the Amazon website, the code locates an element with the id attribute of 'availability' using the find() function. \n",
    "\n",
    "The get_text() function is then used to extract the text content of this element. \n",
    "\n",
    "The .strip() function is again used to remove any extra whitespace characters from the text. \n",
    "\n",
    "The text is then split into words using the .split() function and converted to lowercase. \n",
    "\n",
    "This allows us to check if the word 'out' is present in the text, which indicates that the product is out of stock. Which can be seen in below part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Product prices from the website.\n",
    "\n",
    "if (Flipkart_Availability == 'In stock'):\n",
    "    Flipkart_Price = Flipkart_soup.find_all('div',attrs={\"class\":\"_30jeq3 _16Jk6d\"})[0].get_text()\n",
    "    Flipkart_Price = Flipkart_Price.strip()\n",
    "    Flipkart_Price = Flipkart_Price[1:]\n",
    "    Flipkart_Price = int(Flipkart_Price.replace(',',''))\n",
    "else:\n",
    "    Flipkart_Price = 0\n",
    "\n",
    "\n",
    "if ('out' not in Amazon_Availability_Split):\n",
    "    Amazon_Price = Amazon_soup.find(id='corePriceDisplay_desktop_feature_div').get_text()\n",
    "    Amazon_Price = Amazon_Price.strip()\n",
    "    Amazon_Price = Amazon_Price.split('â‚¹')[1]\n",
    "    Amazon_Price = int(Amazon_Price.replace(',',''))\n",
    "else :\n",
    "    Amazon_Price = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, if the product is in stock on Flipkart (Flipkart_Availability == 'In stock'), the code searches for all 'div' elements that have a 'class' attribute of '_30jeq3 _16Jk6d' using the find_all().\n",
    "\n",
    "The price is then extracted from the text content of the first element in the list, and various string operations are used to clean up the price and convert it to an integer.\n",
    "\n",
    "Similarly, if the product is not in stock on Amazon ('out' in Amazon_Availability_Split), the 'Amazon_Price' variable is set to 0. \n",
    "\n",
    "Otherwise, the code locates an element with the 'id' attribute of 'corePriceDisplay_desktop_feature_div' using the find() function. \n",
    "\n",
    "The price is then extracted from the text content of this element using string operations similar to those used for Flipkart, and the price is converted to an integer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will initliase some variables.\n",
    "\n",
    "1. The file_path variable stores the path of the CSV file that will be used to store the product information. \n",
    "2. We will use the os.path.isfile() function to check if the CSV file already exists. If the file exists, the 'flag' variable is set to 'True'. If not, 'flag' is set to 'False'.\n",
    "3. The 'filename' variable stores the name of the CSV file.\n",
    "4. The 'current_datetime' variable stores the current date and time.\n",
    "5. The 'Amazon_price_list' and 'Flipkart_price_list' variables are empty lists that will be used to store the prices of the product on Amazon and Flipkart websites, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:/ProjectWebScraper/DiscountAlert.csv'\n",
    "\n",
    "#The r prefix is used to create a raw string literal, which makes it easier to include backslashes in the file path.\n",
    "\n",
    "flag = os.path.isfile(file_path)\n",
    "\n",
    "filename = \"DiscountAlert.csv\"\n",
    "current_datetime = datetime.datetime.now()\n",
    "\n",
    "Amazon_price_list = []\n",
    "Flipkart_price_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's first write a function which will help us send email notifications to our mail ids when the prices are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(subject, body, sender, recipients, password):\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = ', '.join(recipients)\n",
    "    smtp_server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    smtp_server.login(sender, password)\n",
    "    smtp_server.sendmail(sender, recipients, msg.as_string())\n",
    "    smtp_server.quit()\n",
    "\n",
    "sender = \"vaidya.kare@gmail.com\"\n",
    "recipients = [\"vishweshhampali@gmail.com\"]\n",
    "password = \"yfvjflmehenezdyl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, we are defining a function 'send_email()' that takes five paramters - 'subject', 'body', 'sender', 'recipients', and 'password'.\n",
    "\n",
    "Inside the function, an email message is created using 'MIMEText()' function from the email package. \n",
    "\n",
    "Then, a secure connection is established with Gmail's SMTP server using 'smtplib.SMTP_SSL()'. The function then logs into the sender's account using 'smtp_server.login()'. Finally, the email message is sent using 'smtp_server.sendmail()', and the connection is closed using 'smtp_server.quit()'.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will develop the program logic store extracted product prices in a CSV file. This will allow us to keep track of the price changes over time. Additionally, we can set up a notification system that compares the current price with the minimum price stored in the CSV file, and sends an email alert if the current price drops to the minimum or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
